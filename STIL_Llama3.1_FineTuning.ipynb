{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOLj+AimVx5BSM38aahVnSf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fO41XinKh--4"},"outputs":[],"source":["#com unsloth para facilitar e reduzir o uso de memória/ser mais rápido"]},{"cell_type":"code","source":["!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"],"metadata":{"id":"KOjyDUQ8jVJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Depedências\n","from unsloth import FastLanguageModel\n","import torch\n","from datasets import load_dataset\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","from transformers import TextStreamer"],"metadata":{"id":"fbO3uPEKjkfZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_seq_length = 2048\n","dtype = None\n","load_in_4bit = True # Quantização 4bit para reduzir o uso de memória\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")"],"metadata":{"id":"NSAf4GkYjrAI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Inferência antes de aplicar o Fine tuning\n","\n","#Estrutura de dado para passar par ao modelo\n","alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model)\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"Crie uma questão de programação em Python, sobre Classificação de Temperatura e que envolva o tópico de decisão simples.\", # instruction\n","        \"\", # input\n","        \"\", # output - sempre em branco para receber a geração\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer)"],"metadata":{"id":"AXsIPhGT4cB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16,\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0,\n","    bias = \"none\",\n","    use_gradient_checkpointing = \"unsloth\",\n","    random_state = 3407,\n","    use_rslora = False,\n","    loftq_config = None,\n",")"],"metadata":{"id":"8zK6ISOspFBp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Formatação e passagem de dados de treinamento\n","EOS_TOKEN = tokenizer.eos_token\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"instruction\"]\n","    inputs       = examples[\"input\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass\n","\n","#local que se encontram os arquivos\n","localFile = \"/content/sample_data/questoes.json\"\n","print(localFile)\n","\n","#carregando conjunto de dados\n","dataset = load_dataset(\"json\", data_files={\"train\":localFile}, split = \"train\")\n","print(dataset)\n","\n","#Aplicando a formatação compatível para o Llama 3.1\n","dataset = dataset.map(formatting_prompts_func, batched = True,)\n","print(dataset)"],"metadata":{"id":"WBHBF3bYpJPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Parâmentros de treino\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False,\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        max_steps = 60,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"],"metadata":{"id":"vP_5MXtqpUUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Realizar o Fine tuning\n","trainer_stats = trainer.train()"],"metadata":{"id":"JLXBuNHat80n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Inferência após o Fine tuning\n","FastLanguageModel.for_inference(model)\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"Crie uma questão de programação em Python, sobre Classificação de Temperatura e que envolva o tópico de decisão simples.\", # instruction\n","        \"\", # input\n","        \"\", # output - sempre em branco para receber a geração\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer,)"],"metadata":{"id":"2RjS8w4ZvJkw"},"execution_count":null,"outputs":[]}]}